{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Inference\n",
    "\n",
    "This article contains random notes about VI-related methods (not all about VI though)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amortized Variational Inference (VAE example)\n",
    "\n",
    "Suppose we have a generative model with $p_\\theta(z,x)=p_\\theta(z)p_\\theta(x|z)$ with N observed instance $x_1,x_2,...,x_N$. Instead of having N variational distributions $q_1(z),q_2(z),...,q_N(z)$ for every instance, we can use an amortized infernece: $q_\\phi(z|x)$ (e.g. there is a neural network parameterized by $\\phi$ that takes $x$ as an input and produce the distribution parameters for $z$)\n",
    "\n",
    "The goal of optimization is, given a true data distribution $p(x)$, for $p_\\theta(x)$ and $q_\\phi(z|x)$ to approximate $p(x)$ and $p_\\theta(z|x)$ respectively.\n",
    "\n",
    "The ELBO of $p_\\theta(x)$ in this model is:\n",
    "\\begin{equation}\n",
    "ELBO(x) = \\log p_\\theta(x)-\\mathrm{KL}[q_\\phi(z|x)||p_\\theta(z|x)]= \\mathbb{E}_{q_\\phi(z|x)}[p_\\theta(x|z)] + \\mathrm{KL}[q_\\phi(z|x)||p_\\theta(z)]\n",
    "\\end{equation}\n",
    "\n",
    "Maximizing $\\mathbb{E}_{p(x)}[ELBO(x)]$ achieves our goal, since \n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\mathbb{E}_{p(x)}[ELBO(x)]&=\\mathbb{E}_{p(x)}[\\log p_\\theta(x)-\\log p(x) + \\log p(x) -\\mathrm{KL}[q_\\phi(z|x)||p_\\theta(z|x)]] \\\\\n",
    "&=\\mathbb{E}_{p(x)}[\\log p(x)]-\\mathrm{KL}[p(x)||p_\\theta(x)] - \\mathbb{E}_{p(x)}[\\mathrm{KL}[q_\\phi(z|x)||p_\\theta(z|x)]]\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, the optimization problem is to maximize $\\mathbb{E}_{p(x)}[\\mathbb{E}_{q_\\phi(z|x)}[p_\\theta(x|z)] + \\mathrm{KL}[q_\\phi(z|x)||p_\\theta(z)]]$ w.r.t. $\\theta$ and $\\phi$. In practice, we use minibatch of data to approximate this expectation.\n",
    "\n",
    "After training, we can use the generative model $p_\\theta(z,x)$ to generate new data (e.g. images).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient estimation\n",
    "\n",
    "In order to train the network described above, we need to estimate the gradient (w.r.t. $\\phi$) of a term of the form $\\mathbb{E}_{q_\\phi(z)}[f(z)]$\n",
    "\n",
    "***1.*** If $q_\\phi(z)$ is parameterizable, i.e. we can rewrite $z=g(\\epsilon,\\phi), \\epsilon \\sim r(\\epsilon)$.\n",
    "\n",
    "Then, we can rewrite $\\mathbb{E}_{q_\\phi(z)}[f(z)]$ to $\\mathbb{E}_{r(\\epsilon)}[f(g(\\epsilon,\\phi))]$ and the gradient can be computed easily using MC sampling (since it is a deterministic function of $\\phi$)\n",
    "\n",
    "***2.*** If $q_\\phi(z)$ is not parameterizable, we can use the REINFORCE estimator.\n",
    "\n",
    "Using the log-derivative trick, we have $\\frac{\\partial q_\\phi(z)}{\\partial \\phi} = q_\\phi(z)\\frac{\\partial \\log q_\\phi(z)}{\\partial \\phi}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{\\partial \\mathbb{E}_{q_\\phi(z)}[f(z)]}{\\partial \\phi} &= \\frac{\\partial \\int q_\\phi(z)f(z)dz}{\\partial \\phi}\\\\\n",
    "&= \\int \\frac{\\partial q_\\phi(z)f(z)}{\\partial \\phi}dz\\\\\n",
    "&= \\int f(z)q_\\phi(z)\\frac{\\partial \\log q_\\phi(z)}{\\partial \\phi}dz\\\\\n",
    "&= \\mathbb{E}_{q_\\phi(z)}\\left[f(z)\\frac{\\partial \\log q_\\phi(z)}{\\partial \\phi}\\right]\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "And this can be approximated with MC sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tighter bound with Importance Sampling\n",
    "\n",
    "Staring from an unbiased estimator of $p_\\theta(x)=\\mathbb{E}_{p_\\theta(x)}[p_\\theta(x|z)]$ by using importance sampling with a proposal $q_\\phi(z|x)$:\n",
    "\\begin{equation}\n",
    "\\frac{1}{K}\\sum_{i=1}^K \\frac{p_\\theta(x,z^i)}{q_\\phi(z^i|x)}, \\quad z^1,z^2,...,z^K \\sim q_\\phi(z|x)\n",
    "\\end{equation}\n",
    "\n",
    "Since this is an unbiased estimator of $p_\\theta(x)$, we have:\n",
    "\\begin{equation}\n",
    "p(x) = \\mathbb{E}_{z^{1..K}\\sim q_\\phi(z|x)}\\left[\\frac{1}{K}\\sum_{i=1}^K \\frac{p_\\theta(x,z^i)}{q_\\phi(z^i|x)}\\right]\n",
    "\\end{equation}\n",
    "\n",
    "Using Jensen Inequality, we have:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\log p(x) &= \\log \\mathbb{E}_{z^{1..K}\\sim q_\\phi(z|x)}\\left[\\frac{1}{K}\\sum_{i=1}^K \\frac{p_\\theta(x,z^i)}{q_\\phi(z^i|x)}\\right]\\\\\n",
    "&\\geq \\mathbb{E}_{z^{1..K}\\sim q_\\phi(z|x)}\\left[\\log\\frac{1}{K}\\sum_{i=1}^K \\frac{p_\\theta(x,z^i)}{q_\\phi(z^i|x)}\\right]\\\\\n",
    "&= ELBO\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "By setting $K=1$, we get the normal ELBO of variational inference described in the above section. The bound gets tighter when we increase $K$.\n",
    "\n",
    "Now we will estimate the gradient w.r.t. $\\phi$ of this ELBO (note that the gradient w.r.t. $\\theta$ is easy to compute).\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "&\\nabla_\\phi\\mathbb{E}_{z^{1..K}\\sim q_\\phi(z|x)}\\left[\\log\\frac{1}{K}\\sum_{i=1}^K \\frac{p_\\theta(x,z^i)}{q_\\phi(z^i|x)}\\right]\\\\\n",
    "&= \\nabla_\\phi\\int \\log\\frac{1}{K}\\sum_{i=1}^K \\frac{p_\\theta(x,z^i)}{q_\\phi(z^i|x)} \\prod_{i=1}^K q_\\phi(z^i|x) dz^{12..K}\\\\\n",
    "&= \\int \\nabla_\\phi\\left[\\log\\frac{1}{K}\\sum_{i=1}^K \\frac{p_\\theta(x,z^i)}{q_\\phi(z^i|x)} \\prod_{i=1}^K q_\\phi(z^i|x)\\right] dz^{12..K}\\\\\n",
    "&= \\int \\nabla_\\phi\\left[\\log\\frac{1}{K}\\sum_{i=1}^K \\frac{p_\\theta(x,z^i)}{q_\\phi(z^i|x)}\\right] \\prod_{i=1}^K q_\\phi(z^i|x) + \\log\\frac{1}{K}\\sum_{i=1}^K \\frac{p_\\theta(x,z^i)}{q_\\phi(z^i|x)} \\nabla_\\phi\\prod_{i=1}^K q_\\phi(z^i|x) \\; dz^{12..K}\\\\\n",
    "&= \\int \\nabla_\\phi\\left[\\log\\frac{1}{K}\\sum_{i=1}^K \\frac{p_\\theta(x,z^i)}{q_\\phi(z^i|x)}\\right] \\prod_{i=1}^K q_\\phi(z^i|x) + \\log\\frac{1}{K}\\sum_{i=1}^K \\frac{p_\\theta(x,z^i)}{q_\\phi(z^i|x)} \\prod_{i=1}^K q_\\phi(z^i|x) \\nabla_\\phi\\log\\prod_{i=1}^K q_\\phi(z^i|x) \\; dz^{12..K}\\\\\n",
    "&= \\mathbb{E}_{z^{1..K}\\sim q_\\phi(z|x)}\\left[\\nabla_\\phi\\log\\frac{1}{K}\\sum_{i=1}^K \\frac{p_\\theta(x,z^i)}{q_\\phi(z^i|x)} + \\log\\frac{1}{K}\\sum_{i=1}^K \\frac{p_\\theta(x,z^i)}{q_\\phi(z^i|x)} \\nabla_\\phi\\log\\prod_{i=1}^K q_\\phi(z^i|x)\\right]\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Then, the REINFORCE estimator of the gradient is\n",
    "\\begin{equation}\n",
    "\\nabla_\\phi\\log\\frac{1}{K}\\sum_{i=1}^K f_{\\phi,\\theta}(x,z^i) + \\log\\frac{1}{K}\\sum_{i=1}^K f_{\\phi,\\theta}(x,z^i) \\nabla_\\phi\\log\\prod_{i=1}^K q_\\phi(z^i|x)\n",
    "\\end{equation}\n",
    "where $f_{\\phi,\\theta}(x,z^i) =  \\frac{p_\\theta(x,z^i)}{q_\\phi(z^i|x)}$\n",
    "\n",
    "The first term:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "&\\nabla_\\phi\\log\\frac{1}{K}\\sum_{i=1}^K f_{\\phi,\\theta}(x,z^i)\\\\\n",
    "&=\\frac{1}{\\sum_{i=1}^K f_{\\phi,\\theta}(x,z^i)}\\sum_{i=1}^K \\nabla_\\phi f_{\\phi,\\theta}(x,z^i)\\\\\n",
    "&=\\sum_{i=1}^K w^i\\nabla_\\phi \\log f_{\\phi,\\theta}(x,z^i)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "where $w^i=\\frac{f_{\\phi,\\theta}(x,z^i)}{\\sum_{k=1}^K f_{\\phi,\\theta}(x,z^k)}$\n",
    "\n",
    "The second term:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "&\\log\\frac{1}{K}\\sum_{i=1}^K f_{\\phi,\\theta}(x,z^i) \\sum_{i=1}^K\\nabla_\\phi\\log q_\\phi(z^i|x)\\\\\n",
    "&=\\sum_{k=1}^K\\left[\\log\\frac{1}{K}\\sum_{i=1}^K f_{\\phi,\\theta}(x,z^i) \\nabla_\\phi\\log q_\\phi(z^k|x)\\right]\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "The first term behaves nicely since it is a linear combination of the gradient of individual sample, with non-negative weights which sum up to 1. Therefore, the term is bounded by the bigest (and smallest) gradient componient. However, in the second term, the weight for individual-sample gradient is a (unbounded) constant. Therefore, this term is potentially unbounded with high variance; and additional measures should be taken to reduce the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
