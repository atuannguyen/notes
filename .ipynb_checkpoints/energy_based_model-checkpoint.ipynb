{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy function\n",
    "For every distribution $p(x)$, we can define the energy function $E(x)=-\\log p(x)$ then $p(x)=\\exp\\left({-E(x)}\\right)$\n",
    "\n",
    "Then, for a point $x$ where $E(x)$ is low, $p(x)$ is high and we can say that it is favored for the variable $X$ to be assigned with value $x$. This is inspired from physics where a particle tends to move to a state with low energy.\n",
    "\n",
    "The energy function can be used to do density estimation of a distribution with observeds data.\n",
    "\n",
    "\n",
    "Let's assume that we have a true (but unknown) distribution $p(x)$ with observed data $x_1,x_2,...,x_n$ and we want to estimate the density of $p(x)$.\n",
    "\n",
    "Let the energy function to be a neural network $E_\\theta(x)$ (parameterized by $\\theta$). Our goal is to tune $\\theta$ so that this energy function estimate the density of the distribution well.\n",
    "\n",
    "We have:\n",
    "\\begin{equation}\n",
    "  p_\\theta(x) = \\frac{\\exp\\left(-E_\\theta(x)\\right)}{Z(\\theta)},\n",
    "\\end{equation}\n",
    "based on the Gibbs distribution where $Z(\\theta)$ is the normalizing constant.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "  \\log p_\\theta(x) &= -E_\\theta(x)-\\log \\left(\\int \\exp\\left(-E_\\theta(x')\\right)dx'\\right)\\\\\n",
    "  \\frac{\\partial\\log p_\\theta(x)}{\\partial\\theta}&=-\\frac{\\partial E_\\theta(x)}{\\partial\\theta} - \\frac{\\partial \\log \\left(\\int \\exp\\left(-E_\\theta(x')\\right)dx'\\right)}{\\partial \\theta} \\\\\n",
    "  &= -\\frac{\\partial E_\\theta(x)}{\\partial\\theta} - \n",
    "          \\frac{1}{\\int \\exp\\left(-E_\\theta(x')\\right)dx'}.\\frac{\\partial \\int \\exp\\left(-E_\\theta(x')\\right)dx'}{\\partial \\theta}\\\\\n",
    "  &= -\\frac{\\partial E_\\theta(x)}{\\partial\\theta} - \n",
    "          \\frac{1}{\\int \\exp\\left(-E_\\theta(x')\\right)dx'}.\\int \\frac{\\partial \\exp\\left(-E_\\theta(x')\\right)}{\\partial \\theta}dx'\\\\\n",
    "  &= -\\frac{\\partial E_\\theta(x)}{\\partial\\theta} - \n",
    "          \\frac{1}{Z(\\theta)}.\\int \\exp\\left(-E_\\theta(x')\\right)\\frac{\\partial -E_\\theta(x')}{\\partial \\theta}dx'\\\\\n",
    "  &= -\\frac{\\partial E_\\theta(x)}{\\partial\\theta} + \n",
    "          \\int \\exp\\left(-E_\\theta(x')\\right)/Z(\\theta)\\frac{\\partial E_\\theta(x')}{\\partial \\theta}dx'\\\\\n",
    "  &= -\\frac{\\partial E_\\theta(x)}{\\partial\\theta} + \n",
    "          \\int p_\\theta(x')\\frac{\\partial E_\\theta(x')}{\\partial \\theta}dx'\\\\\n",
    "  &= -\\frac{\\partial E_\\theta(x)}{\\partial\\theta} + \n",
    "          \\mathbb{E}_{p_\\theta(x')}\\left[\\frac{\\partial E_\\theta(x')}{\\partial \\theta}\\right]\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Then to maximize the data likelihood, our objective function is:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial\\log p_\\theta(D)}{\\partial\\theta}=\\sum_{i=1}^{N} \\frac{\\partial\\log p_\\theta(x_i)}{\\partial\\theta}\n",
    "=-\\sum_{i=1}^{N}\\frac{\\partial E_\\theta(x_i)}{\\partial\\theta} + N.\\mathbb{E}_{p_\\theta(x')}\\left[\\frac{\\partial E_\\theta(x')}{\\partial \\theta}\\right]\n",
    "\\end{equation}\n",
    "\n",
    "The first term can be computed easily, however, the second term is intractable and we have to approximate with samples from $p(x')$. Stochastic Gradient Langevin Dynamics can be used for such sampling.\n",
    "\n",
    "The (intuitive) meaning of the above objective function is that, by maximizing it, we want to minimize $E_\\theta(x_i)$ $\\forall i$ (low energy state) while maximizing $E_\\theta(x')$ of other $x'$'s (high energy state)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your classifier is secretly an energy based model and you should treat it like one, [[ICLR2020]](https://openreview.net/forum?id=Hkxzx0NtDB)\n",
    "\n",
    "In this paper, they showed that learning a discriminative classifier $p_\\theta(y|x)$ is similar to estimate the density of the true distribution of $x,y$ with $p_\\theta(x,y)$ with a energy-based model.\n",
    "\n",
    "First let's assume the number of classes is $K$ $(0\\leq y\\leq K-1)$\n",
    "\n",
    "Define $E_\\theta(x,y)=f_\\theta(x)[y]$ where $f$ is a function that outputs K-dimensional vector.\n",
    "\n",
    "Then:\n",
    "\\begin{equation}\n",
    "\\log p_\\theta(x,y) = \\log p_\\theta(y|x) + \\log p_\\theta(x)\n",
    "\\end{equation}\n",
    "\n",
    "where $p_\\theta(y|x) = \\frac{\\exp(f_\\theta(x)[y])}{\\sum_y^{'}\\exp(f_\\theta(x)[y']}$ \n",
    "\n",
    "and $p_theta(x)=\\frac{\\sum_y \\exp(f_\\theta(x)[y])}{Z(\\theta)}$, or in another word, the enery function w.r.t. $x$ is $E_\\theta(x)=-\\log \\sum_y \\exp(f_\\theta(x)[y])$\n",
    "\n",
    "Here, the first term is exactly the cross entropy loss of a normal classifier. For the second term, we can maximize it by the gradient formular as the above section. Note that we can maximize $\\log p_\\theta(x,y)$ directly by the gradient formular in the above section. However, since we are doing MCMC sampling with a finite number of steps to approximate the final expectation, that estimator is biased. Since we are mainly interested in $p_\\theta(y|x)$, we want to leave it out of that biased estimator and use the estimator to maximize $\\log p_\\theta(x)$ only (which is less important)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hopfield Networks\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
