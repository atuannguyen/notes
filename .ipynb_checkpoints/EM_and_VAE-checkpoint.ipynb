{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation Maximization and Variational Auto-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Suppose that we have a generative model $p_\\theta(x,z) = p_\\theta(z)p_\\theta(x|z)$, with observed variable $x$ ($D=\\{x_1,x_2,...,x_n\\}$) and latent variable $z$. The goal is to learn the parameter $\\theta$ to maximize the (marginal) log likelihood of the observed data (e.g. maximize $\\log p_\\theta(X) = \\sum_{i=1}^n \\log p_\\theta(x_i) = \\sum_{i=1}^n \\log \\int p_\\theta(x_i|z)p_\\theta(z)dz$ ).\n",
    "\n",
    "Expectation Maximization (EM) and Variational Auto-Encoder (VAE) are both methods to solve this problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation Maximization\n",
    "\n",
    "EM is an iterative algorithm that consist of 2 steps: expectation and maximization. The algorithm is:\n",
    "\n",
    "- Initialize $\\theta^{(1)}$\n",
    "- At step $t$:\n",
    "    - Infer the posterior $p_{\\theta^{(t)}}(z|x)$, $x \\in D$\n",
    "    - Choose $\\theta^{(t+1)}$ as:\n",
    "        \\begin{equation}\n",
    "        \\theta^{(t+1)} = argmax_\\theta \\sum_{x\\in D} \\mathbb{E}_{p_{\\theta^{(t)}}(z|x)}[\\log p_\\theta(x,z)]\n",
    "        \\end{equation}\n",
    "        \n",
    "By performing these two steps repeatedly, $\\sum_{x\\in D}\\log p_\\theta(x)$ will converge to a (local) maximum (show proof).\n",
    "\n",
    "Usually, the E step is to compute the expectation $\\mathbb{E}_{p_{\\theta^{(t)}}(z|x)}[\\log p_\\theta(x,z)]$, while the M step is to maximize this expectation. We will write the steps in this equivalent form, though, since it will relate to the VAE algorithm.\n",
    "\n",
    "This algorithm, however, requires $p_\\theta(z|x)$ to be tractable. If it is not, we might have to use approximate distribution (similar to the below VAE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Auto-Encoder\n",
    "\n",
    "In VAE, we introduce an amortized inference network $q_\\phi(z|x)$. The goal is to maximize the Evidence Lowever Bound:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "ELBO &= \\sum_{x \\in D} \\left[\\log p_\\theta(x) - KL[q_\\phi(z|x)||p_\\theta(z|x)]\\right]\\\\\n",
    "&= \\sum_{x \\in D} \\left[\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - KL[q_\\phi(z|x)||p_\\theta(z)]\\right]\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "By maximizing the $ELBO$ w.r.t. $\\theta$ and $\\phi$ simultaneously, we will also maximize $\\sum_{x\\in D} \\log p_\\theta(x)$ (which has lower bound $ELBO$), which is our goal. Also, $q_\\phi(z|x)$ will approximate $p_\\theta(z|x)$\n",
    "\n",
    "See my note on [Variational Inference](https://github.com/tuannguyen-oxford/notes/blob/main/variational_inference.ipynb) for more details on how to estimate the gradient of $ELBO$ w.r.t. $\\theta$ and $\\phi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM and VAE: Similarity\n",
    "\n",
    "Em and VAE both solve the same problem (as described above). Moreover, there are (incredibly) more similarities between them.\n",
    "\n",
    "Let's look at the optimization w.r.t. $\\phi$ and $\\theta$ separately,\n",
    "\n",
    "- When maximizing ELBO w.r.t. $\\phi$, we minimize $KL[q_\\phi(z|x)||p_\\theta(z|x)]$. On another word, $q_\\phi(z|x)$ will approximate $p_\\theta(z|x)$. This is similar to step 1 of EM, but instead of inferring $p_\\theta(z|x)$ directly, we approximate it by $q_\\phi$.\n",
    "- When maximizing ELBO w.r.t. $\\theta$, we maximize $\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)+\\log p_\\theta(z)]=\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x,z)]$. This is similar to step 2 of EM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
